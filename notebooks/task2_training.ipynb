{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, absolute_import\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "#root\n",
    "absPath = '/home/angela/padding_EBI/'\n",
    "sys.path.insert(0, absPath)\n",
    "\n",
    "#from src.Target import Target\n",
    "\n",
    "np.random.seed(8)\n",
    "random.seed(8)\n",
    "\n",
    "from src.preprocessing import *\n",
    "from src.model_architecture import *\n",
    "from src.training_model import *\n",
    "from src.postprocessing import *\n",
    "from src.comparing_results import *\n",
    "#from src.callbacks import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_paddings = [\n",
    "    'post_padding', 'pre_padding', 'mid_padding', 'strf_padding', 'ext_padding', \n",
    "                 'rnd_padding', \n",
    "                 'aug_padding'\n",
    "                \"zoom_padding\"\n",
    "]\n",
    "#list_paddings = ['post_padding', 'pre_padding', 'mid_padding', 'strf_padding', 'ext_padding', \n",
    "#                 'rnd_padding']\n",
    "#list_paddings = [\"aug_padding\"]\n",
    "\n",
    "#hierarchy of folders: annotation/dataset/architecture/n_neurs/task/padding\n",
    "folder = 'EC_number/archaea/1conv/64filts_size5/' #'EC_number/archaea/3denses/bio_neurons/'\n",
    "column = \"EC number\"\n",
    "\n",
    "n_folds = 10\n",
    "\n",
    "dicti = creating_dict()\n",
    "\n",
    "max_lenn = 1000\n",
    "\n",
    "n_class = 7 #number of classes to output\n",
    "drop_per = 0.2 #Input dropout \n",
    "n_neur = [313,76]\n",
    "drop_hid = 0.5\n",
    "dict_size = len(dicti)\n",
    "final_act = \"softmax\"\n",
    "n_filt = 64 #None\n",
    "kernel_size = [5] #None\n",
    "pool_size = None\n",
    "\n",
    "batch_size = 54\n",
    "epochss = 200\n",
    "\n",
    "callbacks_list = [None, None, None, None, False, False, False, False]\n",
    "task = \"task2/\"\n",
    "#architecture = \"only_denses\"\n",
    "architecture = \"conv_dense\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###LOADING ENZYME INDICES BECAUSE THIS IS TASK 2\n",
    "\n",
    "file_idcs_aug = os.path.join(absPath, 'data/', folder, 'idcs_aug_enz.pickle')\n",
    "\n",
    "with open(file_idcs_aug, \"rb\") as input_file:\n",
    "    k_aug_indices = pickle.load(input_file)\n",
    "    \n",
    "file_idcs = os.path.join(absPath, 'data/', folder, 'idcs_data_enz.pickle')\n",
    "\n",
    "with open(file_idcs, \"rb\") as input_file:\n",
    "    splitting_sets = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Deep Learning model - Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000, 26)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000, 26)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1000, 64)          8384      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000, 313)         20345     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1000, 313)         0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000, 76)          23864     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000, 76)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 76000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 532007    \n",
      "=================================================================\n",
      "Total params: 584,600\n",
      "Trainable params: 584,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#model = building_2dense_model_task2(max_lenn, dict_size, n_neur, n_class, drop_per, drop_hid, 'softmax', folder)\n",
    "model = building_1convdense_model_task2(max_lenn, dict_size, n_neur, n_class, drop_per, drop_hid, n_filt, kernel_size, final_act, folder)\n",
    "\n",
    "generators_dict = {} \n",
    "for model_type in list_paddings:\n",
    "    generators_dict[model_type] = trainval_generators(splitting_sets, k_aug_indices, model_type, \n",
    "                                                      folder, batch_size, 'labels_task2', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The `kernel_size` argument must be a tuple of 1 integers. Received: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36mnormalize_tuple\u001b[0;34m(value, n, name)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mvalue_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c3eedabfcad1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                         \u001b[0mn_neur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_hid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_act\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                         \u001b[0mlen_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                         n_filt = None, kernel_size=None, pool_size=None)\n\u001b[0m",
      "\u001b[0;32m/home/angela/padding_EBI/src/training_model.py\u001b[0m in \u001b[0;36mmodel_training\u001b[0;34m(model_type, folder, task, idx, callbacks_list, train_generator, val_generator, architecture, max_len, dict_size, batch_size, n_neur, n_class, drop_per, drop_hid, final_act, epochss, len_train, len_val, n_filt, kernel_size, pool_size)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;31m#deberia haber aqui muchos ifs para elegir el tipo de modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     model = model_choice(architecture, task, folder, max_len, dict_size, n_neur, n_class, drop_per, drop_hid, \n\u001b[0;32m--> 157\u001b[0;31m                             final_act, n_filt=n_filt, kernel_size=kernel_size, pool_size=pool_size)\n\u001b[0m\u001b[1;32m    158\u001b[0m     \u001b[0;31m#writing log file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mlog_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mabsPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/checkpoint/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'log_file.txt'\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/angela/padding_EBI/src/model_architecture.py\u001b[0m in \u001b[0;36mmodel_choice\u001b[0;34m(architecture, task, folder, max_len, dict_size, n_neur, n_class, drop_per, drop_hid, final_act, n_filt, kernel_size, pool_size)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             model = building_1convdense_model_task2(max_len, dict_size, n_neur, n_class, drop_per, \n\u001b[0;32m--> 222\u001b[0;31m                                                     drop_hid, n_filt, kernel_size, final_act, folder)\n\u001b[0m\u001b[1;32m    223\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0marchitecture\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"stack_conv\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"task1/\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/angela/padding_EBI/src/model_architecture.py\u001b[0m in \u001b[0;36mbuilding_1convdense_model_task2\u001b[0;34m(max_len, dict_size, number_neurons, n_class, drop_per, drop_hid, n_filt, kernel_size, final_act, folder)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mdropout_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop_per\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_filt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;31m#Denses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mdense_seq1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_neurons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0mkernel_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank,\n\u001b[0;32m--> 109\u001b[0;31m                                                       'kernel_size')\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'strides'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36mnormalize_tuple\u001b[0;34m(value, n, name)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             raise ValueError('The `' + name + '` argument must be a tuple of ' +\n\u001b[0;32m---> 36\u001b[0;31m                              str(n) + ' integers. Received: ' + str(value))\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_tuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             raise ValueError('The `' + name + '` argument must be a tuple of ' +\n",
      "\u001b[0;31mValueError\u001b[0m: The `kernel_size` argument must be a tuple of 1 integers. Received: None"
     ]
    }
   ],
   "source": [
    "for model_type in list_paddings:\n",
    "    generators = generators_dict[model_type]\n",
    "    for idx,i in enumerate(generators):\n",
    "        if model_type == \"aug_padding\":\n",
    "            i_train, i_val, i_test = k_aug_indices[idx]\n",
    "        else: \n",
    "            i_train, i_val, i_test = splitting_sets[idx]\n",
    "        len_train, len_val, len_test = len(i_train), len(i_val), len(i_test)\n",
    "        train_generator, val_generator = i\n",
    "        model_training(model_type, folder, task, idx, callbacks_list, train_generator, \n",
    "                        val_generator, architecture, max_lenn, dict_size, batch_size, \n",
    "                        n_neur, n_class, drop_per, drop_hid, final_act, epochss, \n",
    "                        len_train, len_val,\n",
    "                        n_filt = None, kernel_size=None, pool_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BORRAR CUANDO COMPRUEBE QUE LO OTRO VA BIEN\n",
    "\n",
    "task = \"task2/\"\n",
    "for model_type in list_paddings:\n",
    "    generators = generators_dict[model_type]\n",
    "    for idx,i in enumerate(generators):\n",
    "        if model_type == \"aug_padding\":\n",
    "            i_train, i_val, i_test = k_aug_indices[idx]\n",
    "        else:\n",
    "            i_train, i_val, i_test = splitting_sets[idx]\n",
    "        len_train, len_val, len_test = len(i_train), len(i_val), len(i_test)\n",
    "        train_generator, val_generator = i\n",
    "        folder_cp = ''.join(string for string in [folder, task, model_type, '/', str(idx)]) \n",
    "        #print(folder_cp)\n",
    "        if not os.path.exists(os.path.join(absPath, 'data/checkpoint/', folder_cp)):\n",
    "            os.makedirs(''.join(string for string in [absPath, 'data/checkpoint/', folder_cp]))\n",
    "\n",
    "        callbacks_list = calling_callbacks(folder_cp, folder, model_type, None, None, None, \n",
    "                                   None, False, False, False, False)\n",
    "        folder_task1 =  ''.join(string for string in [folder, task])\n",
    "        model = building_2dense_model_task2(max_lenn, dict_size, n_neur, n_class, drop_per, drop_hid, 'softmax', folder)\n",
    "        #writing log file \n",
    "        log_file = ''.join(string for string in [absPath, 'data/checkpoint/', folder, task, 'log_file.txt' ]) \n",
    "        f = open(log_file, 'a+')\n",
    "        print('Model type: %s \\n' % model_type, file=f)\n",
    "        print('Fold: %i \\n' % idx, file=f)\n",
    " \n",
    "        start = time.time()\n",
    "        formatted_time = datetime.datetime.now()\n",
    "        print('Starting time: %s \\n' % formatted_time, file=f)\n",
    "        history = model.fit_generator(generator=train_generator, \n",
    "                              validation_data=val_generator,\n",
    "                             steps_per_epoch= int(len_train/batch_size),\n",
    "                              validation_steps=int(len_val/batch_size),\n",
    "                             epochs=epochss,\n",
    "                             callbacks=callbacks_list,\n",
    "                             verbose=1)\n",
    "        end = time.time()\n",
    "        formatted_endtime = datetime.datetime.now()\n",
    "        print('Finishing time: %s \\n' % formatted_endtime, file=f)\n",
    "        count_time(start, end, folder, model_type)\n",
    "        saving_results(history, model_type, folder, idx, True)\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
