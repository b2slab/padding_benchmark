{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, absolute_import\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "#root\n",
    "absPath = '/home/angela/padding_EBI/'\n",
    "sys.path.insert(0, absPath)\n",
    "\n",
    "#from src.Target import Target\n",
    "\n",
    "np.random.seed(8)\n",
    "random.seed(8)\n",
    "\n",
    "from src.preprocessing import *\n",
    "from src.model_architecture import *\n",
    "from src.training_model import *\n",
    "from src.postprocessing import *\n",
    "from src.comparing_results import *\n",
    "#from src.callbacks import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_paddings = [\n",
    "    'post_padding', 'pre_padding', 'mid_padding', 'strf_padding', 'ext_padding', \n",
    "                'rnd_padding', \n",
    "                 'aug_padding',\n",
    "                \"zoom_padding\"\n",
    "]\n",
    "#list_paddings = ['post_padding', 'pre_padding', 'mid_padding', 'strf_padding', 'ext_padding', \n",
    "#                 'rnd_padding']\n",
    "#list_paddings = [\"aug_padding\"]\n",
    "\n",
    "#hierarchy of folders: annotation/dataset/architecture/n_neurs/task/padding\n",
    "folder = 'EC_number/archaea/3denses/bio_neurons2/' #'EC_number/archaea/rnn_conv/256rnn/' #3denses/bio_neurons/'\n",
    "#'EC_number/archaea/stack_conv/10filts_sizeJurtz/'\n",
    "#1conv/64filts_size5/' #'EC_number/archaea/3denses/bio_neurons/'\n",
    "column = \"EC number\"\n",
    "\n",
    "n_folds = 10\n",
    "\n",
    "dicti = creating_dict()\n",
    "\n",
    "max_lenn = 1000\n",
    "\n",
    "n_class = 7 #number of classes to output\n",
    "drop_per = 0.2 #Input dropout \n",
    "n_neur = [313,76]\n",
    "drop_hid = 0.5\n",
    "dict_size = len(dicti)\n",
    "final_act = \"softmax\"\n",
    "n_filt = 10 #64 #None\n",
    "kernel_size = [5] #[1,3,5,9,15]#[5] #None\n",
    "pool_size = 10 #None\n",
    "n_hid = 256 #None\n",
    "\n",
    "batch_size = 54\n",
    "epochss = 200\n",
    "\n",
    "callbacks_list = [None, None, None, None, False, False, False, False]\n",
    "task = \"task2/\"\n",
    "architecture = \"only_denses\"\n",
    "#architecture = \"conv_dense\"\n",
    "#architecture = \"rnn_conv\"\n",
    "#architecture = \"stack_conv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "optimizer = Adam(lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###LOADING ENZYME INDICES BECAUSE THIS IS TASK 2\n",
    "\n",
    "file_idcs_aug = os.path.join(absPath, 'data/', folder, 'idcs_aug_enz.pickle')\n",
    "\n",
    "with open(file_idcs_aug, \"rb\") as input_file:\n",
    "    k_aug_indices = pickle.load(input_file)\n",
    "    \n",
    "file_idcs = os.path.join(absPath, 'data/', folder, 'idcs_data_enz.pickle')\n",
    "\n",
    "with open(file_idcs, \"rb\") as input_file:\n",
    "    splitting_sets = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Deep Learning model - Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000, 26)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000, 26)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000, 313)         8451      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1000, 313)         0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000, 76)          23864     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000, 76)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 76000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 532007    \n",
      "=================================================================\n",
      "Total params: 564,322\n",
      "Trainable params: 564,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = building_2dense_model_task2(max_lenn, dict_size, n_neur, n_class, drop_per, drop_hid, 'softmax', folder)\n",
    "#model = building_1convdense_model_task2(max_lenn, dict_size, n_neur, n_class, drop_per, drop_hid, n_filt, kernel_size, final_act, folder)\n",
    "#model = building_convrnn_model_task2(max_lenn, dict_size, n_neur, n_class, drop_per, drop_hid, \n",
    "#                                 n_filt, kernel_size, pool_size, n_hid, final_act, folder, optimizer)\n",
    "#model = building_stackconv_model_task2(max_lenn, dict_size, n_neur, n_class, drop_per, drop_hid, n_filt, \n",
    "#                                       kernel_size, pool_size, final_act, folder, optimizer)\n",
    "\n",
    "generators_dict = {} \n",
    "for model_type in list_paddings:\n",
    "    generators_dict[model_type] = trainval_generators(splitting_sets, k_aug_indices, model_type, \n",
    "                                                      folder, batch_size, 'labels_task2', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type in list_paddings:\n",
    "    generators = generators_dict[model_type]\n",
    "    for idx,i in enumerate(generators):\n",
    "        if model_type == \"aug_padding\":\n",
    "            i_train, i_val, i_test = k_aug_indices[idx]\n",
    "        else: \n",
    "            i_train, i_val, i_test = splitting_sets[idx]\n",
    "        len_train, len_val, len_test = len(i_train), len(i_val), len(i_test)\n",
    "        train_generator, val_generator = i\n",
    "        model_training(model_type, folder, task, idx, callbacks_list, train_generator, \n",
    "                        val_generator, architecture, max_lenn, dict_size, batch_size, \n",
    "                        n_neur, n_class, drop_per, drop_hid, final_act, epochss, \n",
    "                        len_train, len_val,\n",
    "                        n_filt = n_filt, kernel_size=kernel_size, pool_size=pool_size,\n",
    "                      nhid=n_hid, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
